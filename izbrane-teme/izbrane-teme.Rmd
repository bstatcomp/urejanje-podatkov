---
title: "Izbrane teme"
output: bookdown::gitbook
---

## Izbrana poglavja {-}

### Urejanje kumulativne razpredelnice {-}
Iz repozitorija slovenskega COVID sledilnika (https://github.com/sledilnik/data) prenesemo podatke o kumulativnem številu okuženih glede na spol in starostno kategorijo (*age-cases.csv*). Preberimo datoteko v našo sejo R:

```{r}
library(tidyverse)
df <- read_csv("./data-raw/age-cases.csv")
df
```

Opazimo, da so podatki shranjeni v zelo široki razpredelnici. Vsak stolpec v bistvu hrani 2 spremenljivki -- spol in starost. Pretvorimo podatke v urejeno obliko. Začnimo s pretvorbo v daljšo obliko:

```{r}
df_tidy <- df %>% 
  pivot_longer(cols = starts_with("age"), values_to = "cumulative")
df_tidy
```

Naslednji korak je, da stolpec `name` razdružimo. Izgleda, kot da je ločitveni znak pika, torej uporabimo za separator `\\.`, saj razdružujemo z regularnim izrazom:

```{r}
df_tidy <- df %>% 
  pivot_longer(cols = starts_with("age"), values_to = "cumulative") %>%
  separate(name, into = c("delete1", "sex", "age", "delete2"), "\\.")
df_tidy
```

Dobimo opozorilo. Preverimo v čem je težava, tako da si s funkcijo `View()` ogledamo celoten tibble. V 11. vrstici opazimo prvo težavo, v stolpcu `age` imamo vrednost `todate`. Zakaj je do tega prišlo? Poglejmo imena stolpcev izvirne razpredelnice:

```{r}
colnames(df)
```

Opazimo, da niso vsi stolpci ločeni s 3 pikami. Imamo tudi stolpce, ki povzemajo. Na primer, `age.male.todate` vsebuje vsoto vseh okuženih moških do nekega datuma, torej je vsota stolpcev. V urejenih podatkih ne želimo povzemanj, saj ne želimo mešati posameznih podatkov in njihovih vsot. Povzemanja lahko kasneje izračunamo sami. Zadevo rešimo tako, da iz stolpca `name` v daljši obliki izberemo samo tiste stolpce, ki vsebujejo 3 pike, pri čemer imamo okoli pike vedno nek tekst. Uporabimo `str_detect()` in regularne izraze:

```{r}
df_tidy <- df %>%
  pivot_longer(cols = starts_with("age"), values_to = "cumulative") %>%
  filter(str_detect(name, "[:alpha:]*\\.[:alpha:]*\\.[0-9-]*\\.[:alpha:]*")) %>%
  separate("name", into = c("delete1", "sex", "age", "delete2"), sep = "\\.") %>%
  select(date, sex, age, cumulative)
df_tidy
```

Spremenimo manjkajoče vrednosti v 0:

```{r}
df_tidy <- df %>%
  pivot_longer(cols = starts_with("age"), values_to = "cumulative") %>%
  filter(str_detect(name, "[:alpha:]*\\.[:alpha:]*\\.[0-9-]*\\.[:alpha:]*")) %>%
  separate("name", into = c("delete1", "sex", "age", "delete2"), sep = "\\.") %>%
  select(date, sex, age, cumulative) %>%
  mutate(cumulative = replace_na(cumulative, 0))
df_tidy
```
Preostane nam še pretvorba kumulativnih podatkov v dnevne. Trenutno imamo za vsako kombinacijo spola in starosti podano vsoto okuženih do tistega datuma, na primer:

```{r}
df_tmp <- filter(df_tidy, sex == "female", age == "25-34")
df_tmp$cumulative
```

Kako pretvoriti kumulativne podatke v dnevne? Kumulativnemu vektor moramo enostavno odšteti enak vektor, premaknjen za en korak naprej. Na primer:

```{r}
x_cum <- c(1, 5, 6, 7)
x_dly <- x_cum - c(0, x_cum[1:3])
x_cum
x_dly
```

V R imamo za to funkcijo `diff`:

```{r}
x_cum <- c(1, 5, 6, 7)
x_dly <- c(x_cum[1], diff(x_cum))
x_cum
x_dly
```

Uporabimo sedaj to funkcijo, da dobimo dnevne podatke. Pri tem moramo biti pozorni, da so kumulativne vrednosti podane za vsak spol in starost posebej. Torej moramo podatke tudi grupirati:

```{r}
df_tidy <- df %>%
  pivot_longer(cols = starts_with("age"), values_to = "cumulative") %>%
  filter(str_detect(name, "[:alpha:]*\\.[:alpha:]*\\.[0-9-]*\\.[:alpha:]*")) %>%
  separate("name", into = c("delete1", "sex", "age", "delete2"), sep = "\\.") %>%
  select(date, sex, age, cumulative) %>%
  mutate(cumulative = replace_na(cumulative, 0)) %>%
  group_by(sex, age) %>%
  mutate(daily = c(cumulative[1], diff(cumulative)))
df_tidy
df_tidy %>%
  filter(sex == "female", age == "25-34")
```

Urejene podatke še shranimo v mapo `data-clean`:


```{r}
dir.create("./data-clean/") # Če še ni mape data-clean jo ustvarimo.
write_csv2(df_tidy, "./data-clean/age-cases-tidy.csv")
```


### Iskanje ponovnih bolnišničnih sprejemov - dodajanje spremenljivk glede na množico pogojev {-}

Predpostavimo, da imamo podatke o hospitalizaciija pacientov (TabelaA) in celotno bazo hospitalizacij (TabelaB). V datoteki primer.xlsx se nahajata umetno ustvarjena primera teh dveh tabel in željen rezultat.

Tabeli A, želimo dodati dva stolpca in sicer "Sprejem_DA/NE" tipa faktor in Trajanja_b, ki spremeljivka tabele B, a le pri vrsticah, kjer je novo ustvarjena vrednost spremenjlivke "Sprejem_DA/NE" "Da".

Pogoji za ponoven sprejem so sledeči:
    - V obeh tabelah se mora ID_ZO ujemati.
    - Vrsta storitve v tabeli B mora imeti ključ 7.
    - Ponovni sprejem se je moral zgoditi v 30 dneh po zaključku prvega. 

Preberimo najprej obe tabeli v R. Ker sta obe tabeli na enem list z parametroma `rows` in `cols` določimo točno območje, ki ga želimo prebrati. Nastavimo tudi `detectDates = TRUE`, da paket **openxlsx** pravilno prebere datume, drugače jih bo prebral kot cela števila. Nazadnje še spremenimo tipe spremenljivk v bolj primerne, ker so privzeto večinoma tipa character.

```{r}
library(tidyverse)
library(lubridate)
library(openxlsx)
Sys.setlocale(category = "LC_ALL", locale = "Slovenian_Slovenia.1250")
TabelaA <- tibble(openxlsx::read.xlsx("./data-raw/Primer.xlsx", 
                                     rows = 3:15, cols = 1:8, 
                                     detectDates = TRUE))
TabelaB <- tibble(openxlsx::read.xlsx("./data-raw/Primer.xlsx", 
                                     rows = 21:34, cols = 1:8,
                                     detectDates = TRUE))
TabelaA <- TabelaA %>% mutate(ID_BZ = as.integer(ID_BZ),
                            ID_ZO = as.integer(ID_ZO),
                            Vrsta.storitve = as.integer(Vrsta.storitve),
                            Naziv.storitve = factor(Naziv.storitve, 
                                                       levels = c("NBO", "SPP", "REH", "BOL")),
                            Datum.začetka_a = as.Date(Datum.začetka_a),
                            Datum.zaključka_a = as.Date(Datum.zaključka_a),
                            Leto.zaključka = as.integer(Leto.zaključka),
                            Trajanje_a = as.double(Trajanje_a))

TabelaB <- TabelaB %>% mutate(ID.bolnišničnega.zdravljenja = as.integer(ID.bolnišničnega.zdravljenja),
                            ID_ZO = as.integer(ID_ZO),
                            Vrsta.storitve = as.integer(Vrsta.storitve),
                            Naziv.storitve = factor(Naziv.storitve,
                                                       levels = c("NBO", "SPP", "REH", "BOL")),
                            Datum.začetka_b = as.Date(Datum.začetka_b),
                            Datum.zaključka_b = as.Date(Datum.zaključka_b),
                            Leto.zaključka.BZ = as.integer(Leto.zaključka.BZ),
                            Trajanja_b = as.double(Trajanja_b))

TabelaA
TabelaB

```

Pri spremenljivki `Naziv.storitve` smo ročno nastavili nivoje v obeh tabelah ker:
    - V tabeli A ni vseh vrednostih in bi privzeto R izpustil nevidene vrednosti.
    - S ročnim vnosom zagotovimo, da so tudi vrednosti faktorjev v obeh tabelah enaki.
    
Sedaj, ko smo prebrali podatke najprej samo poiščimo vrstice, katere ustrezajo pogojem za ponovni sprejem.

```{r}
RazsirjenA <- inner_join(TabelaA, TabelaB, by = "ID_ZO", 
                         suffix = c("", "_b"))
RazsirjenA <- RazsirjenA %>% 
    filter(Vrsta.storitve_b == 7) %>%
    filter(Datum.zaključka_a < Datum.začetka_b,
           Datum.začetka_b < Datum.zaključka_a + days(30))
RazsirjenA
```

Najprej smo tabeli združili po vrednosti `ID_ZO`. Tukaj smo uporabili še parameter `suffix`, da smo ohranili prvotna imena iz tabele A, tabeli B pa dodali "_b". Ostala dva pogoja smo implementirali z funkcijo `filter`. V tabeli RazsirjenA, so sedaj v stolpcu ID_BZ vrednosti, pri katerih moramo dodati ponovni sprejem. Tukaj predpostavljamo, da je to primarni ključ.

V zadnjem koraku originalni tabeli A v dveh korakih dodamo manjkajoči spremenljivki.

``` {r}
#Dodajmo sprejeme
TabelaA <- TabelaA %>% mutate("Sprejem_DA/NE" = factor(ID_BZ %in% RazsirjenA$ID_BZ,
                  levels = c(TRUE, FALSE),
                  labels = c("Da", "Ne")))
#Dodajmo trajanja_b
left_join(TabelaA, RazsirjenA %>% select(ID_ZO, Trajanja_b), by = "ID_ZO", suffix = c("", "")) %>%
  mutate(Trajanja_b = replace_na(Trajanja_b, 0)) %>%
  select(ID_BZ, ID_ZO, "Sprejem_DA/NE", Trajanja_b)
```

Pri prvem delu uporabljamo operator %in% za delo z množicami in preverimo ali je `ID_BZ` tabele A v izbranih vrsticah tabele RazsirjenA. Na koncu z levim združevanjem dodamo še vrednosti trajanja_b in tabele B in izpišemo okrajšan rezultat. Za izpis točno željene tabele je potrebno le spremeniti zadnji select in podatke shraniti nazaj v tabelo A.


### Paket data.table {-}
Včasih se srečamo s podatkovnimi množicami, ki imajo veliko število zapisov, na primer milijon, 100 milijonov, ali več. Največja omejitev za delo z velikimi podatkovnimi množicami v R je velikost pomnilnika (ang. memory, RAM), saj R vse podatke hrani v pomnilniku. Za primer, numerični vektor dolžine 100 milijonov zasede 800 MB prostora. V kolikor nam pomnilnik dopušča uporabo podatkov, je naslednja ovira čas procesiranja posameznih operacij. V takšnih primerih je smiselno uporabiti programsko opremo (oziroma pakete), ki so namensko razviti za delo z večjo količino podatkov. V R je temu namenjen paket **data.table**. V tem paketu obstaja objekt `data.table`, ki je drugačna različica `data.frame`. Data table ima tri dimenzije `data.table[i, j, by]`:

- `i` Izbira vrstic in urejanje glede na vrednosti v stolpcih. V dplyr sta za to na voljo glagola `filter()` in `arrange()`.
- `j` Izbira stolpcev, spreminjanje vrednosti stolpcev in povzemanje. V dplyr so za to na voljo glagoli `select()`, `mutate()` in `summarise()`.
- `by` Kako združimo podatke pri operacijah. V dplyr je za to na voljo glagol `group_by()`.

Paketa data.table ne bomo spoznali podrobneje, v kolikor želite več informacij predlagamo ta uvod v paket: https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html. Za diskusijo o razlikah, prednostih in slabostih obeh paketov (dplyr in data.table) predlagamo ogled tega vprašanja (in odgovorov): https://stackoverflow.com/questions/21435339/data-table-vs-dplyr-can-one-do-something-well-the-other-cant-or-does-poorly.

Vsekakor takšnih podatkovnih množic ne smemo zamenjati z velepodatki (big data), kjer so množice tako velike, da standardna programska oprema za delo s podatki z njimi ni zmožna delati. V takih primerih se poslužimo programske opreme za velepodatke, kot so, na primer, Hadoop, MongoDB, Apache Spark, itd.

Paket dplyr lahko razširimo s paketom **dtplyr** (https://github.com/tidyverse/dtplyr), ki ga namestimo z ukazom `install.packages(dtplyr)`. Ta nam omogoča uporabo sintakse paketa dplyr, z učinkovitostjo paketa data.table. To naredi tako, da v ozadju dplyr kodo prevede v data.table kodo. Sicer vseeno deluje nekoliko počasneje kot data.table, ampak je razlika večinoma zanemarljiva. 

Poglejmo si preprost primer, kjer generiramo neke podatke in na njih izvedemo nekaj operacij. Zadevo bomo naredili v dplyr, data.table in dtplyr ter sočasno merili čas izvajanja operacij. Tudi data.table dopušča uporabo operatorja pipe. V paketu dtplyr imamo funkcijo `lazy_dt()` katera omogoči t. i. leno ovrednotenje (lazy evaluation). V dplyr se opreacije izvajajo požrešno, ena za drugo, kot so zapisane. Leno ovrednotenje pa omogoča, da se operacije ne izvedejo, dokler na koncu ne kličemo funkcije `as_tibble()`. To omogoča veliko hitrejšo izvedbo.

```{r}
rm(list = ls())
library(data.table)
library(dtplyr)


# Generirani podatki, da lahko poljubno nastavimo število primerov. ------------
n         <- 100000000
x         <- rnorm(n)
price     <- rnorm(n, 5000, 100)
all_types <- apply(expand.grid(LETTERS, LETTERS, 0:9), 1, paste, collapse=".")
types     <- sample(all_types, n, replace = T)

tib_dplyr  <- tibble(x = x, price = price, type = types)
dt_dtplyr  <- lazy_dt(data.table::data.table(x = x, price = price, 
                                             type = types))
dt_dtable  <- data.table::data.table(x = x, price = price, type = types)


# dplyr ------------------------------------------------------------------------
t1 <- Sys.time()
tib_dplyr %>% 
  group_by(type) %>% 
  summarise(mean_price = mean(price))
t2 <- Sys.time()
t2 - t1


# data.table -------------------------------------------------------------------
t1 <- Sys.time()
dt_dtable[, sum(price), keyby=type]
t2 <- Sys.time()
t2 - t1


# dtplyr -----------------------------------------------------------------------
t1 <- Sys.time()
dt_dtplyr %>% 
  group_by(type) %>% 
  summarise(mean_price = mean(price)) %>%
  as_tibble()
t2 <- Sys.time()
t2 - t1

```

Opazimo, da dtplyr in data.table delujeta veliko hitreje kot dplyr. Koliko hitreje je odvisno od večih parametrov, na primer, koliko podatkov imamo, koliko različnih skupin v združevanju, katere operacije izvajamo in podobno. Za bolj podrobno časovno primerjavo predlagamo obisk strani https://github.com/Rdatatable/data.table/wiki/Benchmarks-%3A-Grouping in https://iyarlin.github.io/2020/05/26/dtplyr_benchmarks/. 

Še ena primerjava kode med paketi:

```{r}
# # dplyr ------------------------------------------------------------------------
# t1 <- Sys.time()
# tib_dplyr %>%
#   filter(x < 2) %>%
#   group_by(type) %>%
#   mutate(price_stand = (price - mean(price)) / sd(price)) %>%
#   summarize(mean_price_stand = mean(price_stand))
# t2 <- Sys.time()
# t2 - t1
# 
# 
# # data.table -------------------------------------------------------------------
# t1 <- Sys.time()
# dt_dtable %>% 
#   .[x < 2] %>% 
#   .[ , price_stand := (price - mean(price)) / sd(price), by = type] %>% 
#   .[ , .(mean_price_stand = mean(price_stand)), keyby = type]
# t2 <- Sys.time()
# t2 - t1
# 
# 
# # dtplyr -----------------------------------------------------------------------
# t1 <- Sys.time()
# dt_dtplyr %>%
#   filter(x < 2) %>%
#   group_by(type) %>%
#   mutate(price_stand = (price - mean(price)) / sd(price)) %>%
#   summarize(mean_price_stand = mean(price_stand)) %>%
#   as_tibble() # To vrstico moramo zapisati, v koliko želimo da se ukaz izvede!
# t2 <- Sys.time()
# t2 - t1
```

V tem primeru so časovne razlike zanemarljive. Učinkovitost je odvisna tudi od tipa operacij, ki jih izvajamo.


### Povezava med R in podatkovnimi bazami  {-}
Pogosto podatke pridobimo iz podatkovnih baz, ki so namenjene hranjenju in analizi večjih podatkovnih zbirk. Za delo s podatkovnimi bazami običajno uporabljamo programski jezik SQL. V R obstaja več paketov, ki nam omogočajo povezavo na podatkovno bazo. Najbolj popularni so **DBI**, **odbc** in **dbplyr** (slednja oba temeljita na DBI). Ti paketi delujejo z uveljavljenimi sistemi za upravljanje podatkovnih baz, kot so MySQL, PostgreSQL in SQLite ter veliko komercialnimi podatkovnimi bazami.

Poglejmo si primer uporabe podatkovne baze. Na pomnilniku bomo ustvarili začasno bazo. V praksi bi se s tem povezali na poljubno bazo, več informacij lahko poiščete na: https://db.rstudio.com/.

```{r}
library(dbplyr)
con <- dbConnect(RSQLite::SQLite(), ":memory:") # V pomnilniku ustvarimo začasno bazo.
dbWriteTable(con, "age-cases-clean", df_tidy) # Shranimo podatke v bazo.
df_tidy_from_db <- dbReadTable(con, "age-cases-clean") # Preberemo podatke iz baze v R.
df_tidy_from_db
```

Paket dbplyr nam omogoča uporabo sintakse dplyr kar na bazi. Interno bo prevedel dplyr kodo v SQL in poizvedbe (transformacije) izvedel direktno na bazi. S tem lahko potem upravljamo tudi z večjo količino podatkov, kot pa jih lahko naložimo v R! Poglejmo si primer:

```{r}
razpredelnica_na_bazi <- tbl("age-cases-clean") # S tbl() dostopamo do razpredelnice na bazi.
razpredelnica_na_bazi
```

Opazimo, da dostopamo do razpredelnice naravnost v bazi. Izračunajmo sedaj vsoto dnevnih obolelih za spol in starost kar na bazi. Najprej pripravimo dplyr kodo:

```{r}
povzemi <- razpredelnica_na_bazi %>%
  group_by(sex, age) %>%
  summarise(sum(daily))
povzemi %>% show_query() # Prikaže prevedeno kodo v SQL.
```

Preostane nam še, da izvedemo te operacije na bazi. S `collect()` zberemo rezultate:

```{r}
povzetek <- povzemi %>% collect()
povzetek
dbDisconnect(con) # Zapremo povezavo do baze.
```


### Paralelizacija v R {-}
Pogosto se pri delu s podatki srečujemo z nalogami, ki zahtevajo časovno potratno obdelavo. Dober primer tega je učenje metod umetne inteligence. Že relativno preprosti primeri, kot je linearna regresija, lahko trajajo tudi po več ur, če imamo veliko podatkov. Velikokrat potem modele učimo na različnih podmnožicah. Ampak dva modela učena na različnih množicah nimata praktično nič skupnega, torej bi jih načeloma lahko učili hkrati! V danajšnih dneh večina računalnikov premore več procesorskih jeder, ki so namenjena izvajanju operacij, oziroma računanju. Torej lahko te ločene probleme enostavno razdelimo med več procesorjev in bodo ti naloge opravljali hkrati! Na prenosnikih ni nenavadno, da imamo 4 jedra, torej lahko hkrati poženemo 4 procese. Če vsak traja 1 uro, potem tako prihranimo 3 ure! Boljši računalniki imajo tudi več jeder, na primer 32. Če gremo še dlje, lahko vzporedno izvajanje prenesemo na grafične procesne enote (grafične kartice), ki pa imajo tudi nad 8000 procesorskih enot. 

Tukaj si bomo pogledali relativno preprost primer paralelizacije, kjer bomo zadeve izvajali vzporedno na procesorskih jedrih glavnega procesorja. Koda je pripravljena tako, da lahko število podatkov poljubno povečamo, in s tem primerjamo časovno zahtevnost obeh pristopov.

Kot primer si bomo pogledali relativno preprost statistični model -- linearno regresijo, ki jo bomo učili na dveh ločenih podatkovnih množicah.

``` {r}
set.seed(1) # Zagotovimo ponovljivost.
n <- 1000000 # Število primerov v podatkih.

# Generiramo podatke.
x1 <- rnorm(n)
y1 <- rnorm(n, 4 + 2 * x1, 0.2)
x2 <- rnorm(n)
y2 <- rnorm(n, 3 - 1.5 * x2, 0.2)

df1 <- data.frame(x = x1, y = y1)
df2 <- data.frame(x = x2, y = y2)
df_list <- list(df1, df2)
```

Da poženemo model na vseh pdoatkih, bomo uporabili zanko for skozi vse elemente seznama `df_list()`:

``` {r}
t1 <- Sys.time() # Za izračun potrebnega časa.
my_lms <- list() # V ta seznam bomo shranili rezultate.
for (i in 1:length(df_list)) {
  print(str_c("Računam model: ", i))
  my_lms[[i]] <- lm(y ~ x, data = df_list[[i]])
}
t2 <- Sys.time()
my_lms
t2 - t1
```

Sedaj pa naredimo enako, ampak tako da bomo uporabili 2 procesorski jedri hkrati. Uporabili bomo paket `doParallel`. Sintaksa kode je zelo podobna standardni R zanki:

``` {r}
library(doParallel)
detectCores() # Preverimo, koliko procesorjev imamo na voljo.
nc <- 2 # Število procesorjev.
mc <- makeCluster(nc) # Ustvarimo cluster 2 procesorjev.

# S spodnjim klicem bomo ustvarili log datoteko, kamor se bodo zapisovale
# informacije iz vsakega procesorja.
clusterEvalQ(mc, sink(paste0("./log", Sys.getpid(), ".txt")))

registerDoParallel(mc) # Registriramo cluster.

my_lms <- list()
t1 <- Sys.time() # Za izračun potrebnega časa.
foreach(i = 1:length(df_list)) %dopar% {
  library(stringr) # V vsakem procesorju je potrebno naložiti pakete posebej!
  print(str_c("Računam model: ", i))
  my_lms[[i]] <- lm(y ~ x, data = df_list[[i]])
}
stopCluster(mc) # Ustavimo cluster.
t2 <- Sys.time() # Za izračun potrebnega časa.
my_lms
t2 - t1
```

Predlagamo, da poizkusite tudi sami, ampak z več podatki. Pri tem najprej preverite, koliko jeder ima vaš računalnik, potrebovali boste vsaj 2. Glede na to, da imamo samo 2 razpredelnici, več kot 2 jeder ni smiselno uporabiti.
